{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LXw1gssjz9yj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import time\n",
    "from torchsummary import summary as summary_\n",
    "\n",
    "# 코드 다시 돌리기 위한 seed 고정\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 baseline 모델에서 정확도를 높이기 위해 찾아본 결과 resnet을 적용해보기로 하였다.\n",
    "  \n",
    "ResNet은 Residual neural network로 이름처럼 잔차(residual)과 관련이 있는 모델이다.\n",
    "이 모델은 기존의 방식으로 신경망 층을 깊게 쌓는다고 해서 성능이 좋아지는 것은 아님을 실제로 확인하였다. 따라서 Residual Block을 도입하여 기존의 망과 차이가 있다면 입력값을 출력값에 더해줄 수 있도록 하나의 지름길(shortcut)을 만들어주었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "])        \n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "])    \n",
    "\n",
    "train = torchvision.datasets.CIFAR100(root=\"./\", train=True, download=True, transform=train_transform)\n",
    "test = torchvision.datasets.CIFAR100(root=\"./\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=256,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=256,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet을 18층, 50층짜리 2가지 종류로 만들어 직접 모델을 돌려보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2          [256, 64, 32, 32]             128\n",
      "            Conv2d-3          [256, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4          [256, 64, 32, 32]             128\n",
      "            Conv2d-5          [256, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6          [256, 64, 32, 32]             128\n",
      "        BasicBlock-7          [256, 64, 32, 32]               0\n",
      "            Conv2d-8          [256, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9          [256, 64, 32, 32]             128\n",
      "           Conv2d-10          [256, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11          [256, 64, 32, 32]             128\n",
      "       BasicBlock-12          [256, 64, 32, 32]               0\n",
      "           Conv2d-13         [256, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-14         [256, 128, 16, 16]             256\n",
      "           Conv2d-15         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-16         [256, 128, 16, 16]             256\n",
      "           Conv2d-17         [256, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-18         [256, 128, 16, 16]             256\n",
      "       BasicBlock-19         [256, 128, 16, 16]               0\n",
      "           Conv2d-20         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21         [256, 128, 16, 16]             256\n",
      "           Conv2d-22         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23         [256, 128, 16, 16]             256\n",
      "       BasicBlock-24         [256, 128, 16, 16]               0\n",
      "           Conv2d-25           [256, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-26           [256, 256, 8, 8]             512\n",
      "           Conv2d-27           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-28           [256, 256, 8, 8]             512\n",
      "           Conv2d-29           [256, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-30           [256, 256, 8, 8]             512\n",
      "       BasicBlock-31           [256, 256, 8, 8]               0\n",
      "           Conv2d-32           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-33           [256, 256, 8, 8]             512\n",
      "           Conv2d-34           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-35           [256, 256, 8, 8]             512\n",
      "       BasicBlock-36           [256, 256, 8, 8]               0\n",
      "           Conv2d-37           [256, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38           [256, 512, 4, 4]           1,024\n",
      "           Conv2d-39           [256, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40           [256, 512, 4, 4]           1,024\n",
      "           Conv2d-41           [256, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42           [256, 512, 4, 4]           1,024\n",
      "       BasicBlock-43           [256, 512, 4, 4]               0\n",
      "           Conv2d-44           [256, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45           [256, 512, 4, 4]           1,024\n",
      "           Conv2d-46           [256, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47           [256, 512, 4, 4]           1,024\n",
      "       BasicBlock-48           [256, 512, 4, 4]               0\n",
      "           Linear-49                 [256, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,220,132\n",
      "Trainable params: 11,220,132\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 2880.20\n",
      "Params size (MB): 42.80\n",
      "Estimated Total Size (MB): 2926.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary_(model,(3,32,32),batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for image, label in train_loader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        prediction = output.max(1, keepdim = True)[1]\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "      \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item() \n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 5], \tTrain Loss: 1.9320, \tTrain Accuracy: 47.54 %, \tVal Loss: 2.2521, \tVal Accuracy: 41.99 % \n",
      "\n",
      "[EPOCH: 10], \tTrain Loss: 0.2887, \tTrain Accuracy: 91.03 %, \tVal Loss: 2.9028, \tVal Accuracy: 46.54 % \n",
      "\n",
      "[EPOCH: 15], \tTrain Loss: 0.0845, \tTrain Accuracy: 97.41 %, \tVal Loss: 3.4352, \tVal Accuracy: 47.59 % \n",
      "\n",
      "[EPOCH: 20], \tTrain Loss: 0.0610, \tTrain Accuracy: 98.09 %, \tVal Loss: 3.6661, \tVal Accuracy: 47.41 % \n",
      "\n",
      "[EPOCH: 25], \tTrain Loss: 0.0697, \tTrain Accuracy: 97.85 %, \tVal Loss: 3.6625, \tVal Accuracy: 48.40 % \n",
      "\n",
      "[EPOCH: 30], \tTrain Loss: 0.1236, \tTrain Accuracy: 95.97 %, \tVal Loss: 3.9380, \tVal Accuracy: 45.92 % \n",
      "\n",
      "[EPOCH: 35], \tTrain Loss: 0.0367, \tTrain Accuracy: 98.86 %, \tVal Loss: 4.2864, \tVal Accuracy: 47.27 % \n",
      "\n",
      "[EPOCH: 40], \tTrain Loss: 0.0327, \tTrain Accuracy: 98.98 %, \tVal Loss: 4.2173, \tVal Accuracy: 48.28 % \n",
      "\n",
      "[EPOCH: 45], \tTrain Loss: 0.0425, \tTrain Accuracy: 98.64 %, \tVal Loss: 4.2272, \tVal Accuracy: 47.01 % \n",
      "\n",
      "[EPOCH: 50], \tTrain Loss: 0.0638, \tTrain Accuracy: 98.03 %, \tVal Loss: 4.4591, \tVal Accuracy: 46.63 % \n",
      "\n",
      "[EPOCH: 55], \tTrain Loss: 0.0508, \tTrain Accuracy: 98.41 %, \tVal Loss: 4.8125, \tVal Accuracy: 45.87 % \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# output과 label(정답)과의 criterion으로 loss를 계산(10개의 클래스이므로 Cross Entropy 사용)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 기울기 계산(back propagation을 통해 gradient 계산)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_loader)\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"[EPOCH: {epoch}], \\tTrain Loss: {train_loss:.4f}, \\tTrain Accuracy: {train_accuracy:.2f} %, \\tVal Loss: {val_loss:.4f}, \\tVal Accuracy: {val_accuracy:.2f} % \\n\")\n",
    "    \n",
    "    result = {\n",
    "        'EPOCH': epoch,\n",
    "        'Train Loss': train_loss,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Val Loss':val_loss,\n",
    "        'Val Accuracy': val_accuracy}\n",
    "  \n",
    "    result_list.append(result)\n",
    "result_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 baseline 모델에서는 100 epoch를 돌렸을 때 약 20%의 성능이 나왔다면 resnet18의 모델은 약 48%로 2배이상이 증가함을 확인할 수 있다.\n",
    "\n",
    "(노트북에서 모델을 돌렸을 때 5 epcoh마다 2시간씩 걸려서 중간에 멈췄습니다.. 더군다나 이번 과제의 경우 60퍼센트 이상의 val acc을 나타냈어야 했기 때문에 55 epoch에서 중단한 후 더 깊은 층을 가진 resnet50을 돌려보습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2          [256, 64, 32, 32]             128\n",
      "            Conv2d-3          [256, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4          [256, 64, 32, 32]             128\n",
      "            Conv2d-5          [256, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6          [256, 64, 32, 32]             128\n",
      "            Conv2d-7         [256, 256, 32, 32]          16,384\n",
      "       BatchNorm2d-8         [256, 256, 32, 32]             512\n",
      "            Conv2d-9         [256, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10         [256, 256, 32, 32]             512\n",
      "       Bottleneck-11         [256, 256, 32, 32]               0\n",
      "           Conv2d-12          [256, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-13          [256, 64, 32, 32]             128\n",
      "           Conv2d-14          [256, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15          [256, 64, 32, 32]             128\n",
      "           Conv2d-16         [256, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-17         [256, 256, 32, 32]             512\n",
      "       Bottleneck-18         [256, 256, 32, 32]               0\n",
      "           Conv2d-19          [256, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [256, 64, 32, 32]             128\n",
      "           Conv2d-21          [256, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22          [256, 64, 32, 32]             128\n",
      "           Conv2d-23         [256, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24         [256, 256, 32, 32]             512\n",
      "       Bottleneck-25         [256, 256, 32, 32]               0\n",
      "           Conv2d-26         [256, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-27         [256, 128, 32, 32]             256\n",
      "           Conv2d-28         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29         [256, 128, 16, 16]             256\n",
      "           Conv2d-30         [256, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-31         [256, 512, 16, 16]           1,024\n",
      "           Conv2d-32         [256, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-33         [256, 512, 16, 16]           1,024\n",
      "       Bottleneck-34         [256, 512, 16, 16]               0\n",
      "           Conv2d-35         [256, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-36         [256, 128, 16, 16]             256\n",
      "           Conv2d-37         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38         [256, 128, 16, 16]             256\n",
      "           Conv2d-39         [256, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-40         [256, 512, 16, 16]           1,024\n",
      "       Bottleneck-41         [256, 512, 16, 16]               0\n",
      "           Conv2d-42         [256, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-43         [256, 128, 16, 16]             256\n",
      "           Conv2d-44         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-45         [256, 128, 16, 16]             256\n",
      "           Conv2d-46         [256, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-47         [256, 512, 16, 16]           1,024\n",
      "       Bottleneck-48         [256, 512, 16, 16]               0\n",
      "           Conv2d-49         [256, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50         [256, 128, 16, 16]             256\n",
      "           Conv2d-51         [256, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-52         [256, 128, 16, 16]             256\n",
      "           Conv2d-53         [256, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54         [256, 512, 16, 16]           1,024\n",
      "       Bottleneck-55         [256, 512, 16, 16]               0\n",
      "           Conv2d-56         [256, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-57         [256, 256, 16, 16]             512\n",
      "           Conv2d-58           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59           [256, 256, 8, 8]             512\n",
      "           Conv2d-60          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-61          [256, 1024, 8, 8]           2,048\n",
      "           Conv2d-62          [256, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-63          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-64          [256, 1024, 8, 8]               0\n",
      "           Conv2d-65           [256, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-66           [256, 256, 8, 8]             512\n",
      "           Conv2d-67           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68           [256, 256, 8, 8]             512\n",
      "           Conv2d-69          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-70          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-71          [256, 1024, 8, 8]               0\n",
      "           Conv2d-72           [256, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-73           [256, 256, 8, 8]             512\n",
      "           Conv2d-74           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-75           [256, 256, 8, 8]             512\n",
      "           Conv2d-76          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-77          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-78          [256, 1024, 8, 8]               0\n",
      "           Conv2d-79           [256, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-80           [256, 256, 8, 8]             512\n",
      "           Conv2d-81           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-82           [256, 256, 8, 8]             512\n",
      "           Conv2d-83          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-85          [256, 1024, 8, 8]               0\n",
      "           Conv2d-86           [256, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-87           [256, 256, 8, 8]             512\n",
      "           Conv2d-88           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-89           [256, 256, 8, 8]             512\n",
      "           Conv2d-90          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-91          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-92          [256, 1024, 8, 8]               0\n",
      "           Conv2d-93           [256, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-94           [256, 256, 8, 8]             512\n",
      "           Conv2d-95           [256, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-96           [256, 256, 8, 8]             512\n",
      "           Conv2d-97          [256, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98          [256, 1024, 8, 8]           2,048\n",
      "       Bottleneck-99          [256, 1024, 8, 8]               0\n",
      "          Conv2d-100           [256, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-101           [256, 512, 8, 8]           1,024\n",
      "          Conv2d-102           [256, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-103           [256, 512, 4, 4]           1,024\n",
      "          Conv2d-104          [256, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-105          [256, 2048, 4, 4]           4,096\n",
      "          Conv2d-106          [256, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-107          [256, 2048, 4, 4]           4,096\n",
      "      Bottleneck-108          [256, 2048, 4, 4]               0\n",
      "          Conv2d-109           [256, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-110           [256, 512, 4, 4]           1,024\n",
      "          Conv2d-111           [256, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-112           [256, 512, 4, 4]           1,024\n",
      "          Conv2d-113          [256, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-114          [256, 2048, 4, 4]           4,096\n",
      "      Bottleneck-115          [256, 2048, 4, 4]               0\n",
      "          Conv2d-116           [256, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-117           [256, 512, 4, 4]           1,024\n",
      "          Conv2d-118           [256, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-119           [256, 512, 4, 4]           1,024\n",
      "          Conv2d-120          [256, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-121          [256, 2048, 4, 4]           4,096\n",
      "      Bottleneck-122          [256, 2048, 4, 4]               0\n",
      "          Linear-123                 [256, 100]         204,900\n",
      "================================================================\n",
      "Total params: 23,705,252\n",
      "Trainable params: 23,705,252\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 15839.80\n",
      "Params size (MB): 90.43\n",
      "Estimated Total Size (MB): 15933.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cts08\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "summary_(model,(3,32,32),batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 1], \tTrain Loss: 4.4882, \tTrain Accuracy: 3.37 %, \tVal Loss: 4.1415, \tVal Accuracy: 5.46 % \n",
      "\n",
      "[EPOCH: 2], \tTrain Loss: 3.9648, \tTrain Accuracy: 7.91 %, \tVal Loss: 3.8317, \tVal Accuracy: 10.86 % \n",
      "\n",
      "[EPOCH: 3], \tTrain Loss: 3.6366, \tTrain Accuracy: 12.94 %, \tVal Loss: 3.4984, \tVal Accuracy: 15.66 % \n",
      "\n",
      "[EPOCH: 4], \tTrain Loss: 3.3053, \tTrain Accuracy: 19.14 %, \tVal Loss: 3.2328, \tVal Accuracy: 20.33 % \n",
      "\n",
      "[EPOCH: 5], \tTrain Loss: 2.9965, \tTrain Accuracy: 24.71 %, \tVal Loss: 2.9559, \tVal Accuracy: 26.01 % \n",
      "\n",
      "[EPOCH: 6], \tTrain Loss: 2.6611, \tTrain Accuracy: 31.43 %, \tVal Loss: 2.6054, \tVal Accuracy: 32.84 % \n",
      "\n",
      "[EPOCH: 7], \tTrain Loss: 2.3653, \tTrain Accuracy: 37.40 %, \tVal Loss: 2.3864, \tVal Accuracy: 37.52 % \n",
      "\n",
      "[EPOCH: 8], \tTrain Loss: 2.1254, \tTrain Accuracy: 42.82 %, \tVal Loss: 2.3393, \tVal Accuracy: 39.79 % \n",
      "\n",
      "[EPOCH: 9], \tTrain Loss: 1.9075, \tTrain Accuracy: 47.73 %, \tVal Loss: 2.1813, \tVal Accuracy: 42.83 % \n",
      "\n",
      "[EPOCH: 10], \tTrain Loss: 1.7139, \tTrain Accuracy: 52.44 %, \tVal Loss: 2.0449, \tVal Accuracy: 45.83 % \n",
      "\n",
      "[EPOCH: 11], \tTrain Loss: 1.5302, \tTrain Accuracy: 56.77 %, \tVal Loss: 1.9290, \tVal Accuracy: 48.37 % \n",
      "\n",
      "[EPOCH: 12], \tTrain Loss: 1.3368, \tTrain Accuracy: 61.46 %, \tVal Loss: 2.0559, \tVal Accuracy: 47.60 % \n",
      "\n",
      "[EPOCH: 13], \tTrain Loss: 1.1414, \tTrain Accuracy: 66.70 %, \tVal Loss: 1.9687, \tVal Accuracy: 49.26 % \n",
      "\n",
      "[EPOCH: 14], \tTrain Loss: 0.9357, \tTrain Accuracy: 72.06 %, \tVal Loss: 2.0048, \tVal Accuracy: 49.76 % \n",
      "\n",
      "[EPOCH: 15], \tTrain Loss: 0.7375, \tTrain Accuracy: 77.61 %, \tVal Loss: 2.1373, \tVal Accuracy: 50.89 % \n",
      "\n",
      "[EPOCH: 16], \tTrain Loss: 0.5392, \tTrain Accuracy: 83.37 %, \tVal Loss: 2.2140, \tVal Accuracy: 50.92 % \n",
      "\n",
      "[EPOCH: 17], \tTrain Loss: 0.4466, \tTrain Accuracy: 86.07 %, \tVal Loss: 2.4764, \tVal Accuracy: 49.39 % \n",
      "\n",
      "[EPOCH: 18], \tTrain Loss: 0.3110, \tTrain Accuracy: 90.33 %, \tVal Loss: 2.4287, \tVal Accuracy: 50.32 % \n",
      "\n",
      "[EPOCH: 19], \tTrain Loss: 0.2230, \tTrain Accuracy: 93.17 %, \tVal Loss: 2.6113, \tVal Accuracy: 52.02 % \n",
      "\n",
      "[EPOCH: 20], \tTrain Loss: 0.1542, \tTrain Accuracy: 95.32 %, \tVal Loss: 2.5697, \tVal Accuracy: 51.93 % \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[EPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %, \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mVal Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m % \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# output과 label(정답)과의 criterion으로 loss를 계산(10개의 클래스이므로 Cross Entropy 사용)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 기울기 계산(back propagation을 통해 gradient 계산)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_loader)\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"[EPOCH: {epoch}], \\tTrain Loss: {train_loss:.4f}, \\tTrain Accuracy: {train_accuracy:.2f} %, \\tVal Loss: {val_loss:.4f}, \\tVal Accuracy: {val_accuracy:.2f} % \\n\")\n",
    "    \n",
    "    result = {\n",
    "        'EPOCH': epoch,\n",
    "        'Train Loss': train_loss,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Val Loss':val_loss,\n",
    "        'Val Accuracy': val_accuracy}\n",
    "  \n",
    "    result_list.append(result)\n",
    "result_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50에서는 모델을 돌렸을 때 1 epoch 결과를 출력할 때까지의 시간이 2시간정도 걸렸다. 때문에 이 역시도 100 epoch까지 돌려볼 시간이 부족했지만 20 epoch까지의 결과만 보더라도 앞서 돌렸던 resnet18보다 더 높은 성능(val acc : 50%)을 보여주고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar-100 데이터셋에 대해서 60% 이상의 성능을 보여주기 위해서 기존의 baseline model보다 더 좋은 모델들을 찾아보고 실행해보았다. 특히 resnet으로 모델의 성능을 직접 출력하도록 해보았고 최종적으로 resnet50에 대해서는 20 epoch까지만 하더라도 50%의 성능을 끌어올릴 수 있었다.  \n",
    "따라서 이보다 더 깊은 층의 resnet101 혹은 resnet152에 대해서는 분명 60% 이상의 성능을 끌어올리기 충분할 것이라고 판단한다. 또한 resnet50에 대해서도 적절하게 파라미터를 수정해준다면 충분히 높은 성능을 보일 것이라고 생각한다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "cad004fe989bd74e5c5aec7ffae436e77866a70d082a239967f7d52eef71aa88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
